{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68beb361",
   "metadata": {},
   "source": [
    "# **Load, Split, Embed, Store using OpenAI and Chroma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4023c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\burha\\miniconda3\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from evaluate) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\burha\\miniconda3\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from evaluate) (2.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from evaluate) (1.26.3)\n",
      "Requirement already satisfied: dill in c:\\users\\burha\\miniconda3\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\burha\\miniconda3\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\burha\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\burha\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\burha\\miniconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\burha\\miniconda3\\lib\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from rouge_score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: joblib in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from nltk->rouge_score) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.1/61.1 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from bert_score) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from bert_score) (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from bert_score) (1.26.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from bert_score) (4.35.2)\n",
      "Requirement already satisfied: requests in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from bert_score) (2.31.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from bert_score) (2.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\burha\\miniconda3\\lib\\site-packages (from bert_score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from bert_score) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
      "Requirement already satisfied: sympy in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.0.0->bert_score) (4.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\burha\\miniconda3\\lib\\site-packages (from torch>=1.0.0->bert_score) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.31.1->bert_score) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=3.0.0->bert_score) (0.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=3.0.0->bert_score) (0.15.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=3.0.0->bert_score) (0.20.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from matplotlib->bert_score) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from matplotlib->bert_score) (4.39.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from matplotlib->bert_score) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from matplotlib->bert_score) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->bert_score) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\burha\\miniconda3\\lib\\site-packages (from matplotlib->bert_score) (1.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests->bert_score) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests->bert_score) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests->bert_score) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from requests->bert_score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\burha\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Installing collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement bleu_score (from versions: none)\n",
      "ERROR: No matching distribution found for bleu_score\n"
     ]
    }
   ],
   "source": [
    "#!pip -q install langchain openai chromadb sentence_transformers evaluate rouge_score bert_score bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b0ad0",
   "metadata": {},
   "source": [
    "## **OpenAI Authenticatation**\n",
    "We use OpenAI for the embedding. Make sure to have balance on your OpenAI Dashboard and create a personal secret key at https://platform.openai.com/api-keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa6daf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bdd4550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burha\\miniconda3\\lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "# InstructorEmbedding\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee644af",
   "metadata": {},
   "source": [
    "## **Load documents**\n",
    "All our documents are retrieved by the Scraper/PubMedScraper.py. Executing this script will generate a papers.json.\n",
    "We noad load the json file, and take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059201b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': {'full_text': 'probability estimation with machine learning methods for dichotomous and multicategory outcome: theory.'}, 'abstract': {'full_text': 'probability estimation for binary and multicategory outcome using logistic and multinomial logistic regression has a long-standing tradition in biostatistics. however, biases may occur if the model is misspecified. in contrast, outcome probabilities for individuals can be estimated consistently with machine learning approaches, including k-nearest neighbors (k-nn), bagged nearest neighbors (b-nn), random forests (rf), and support vector machines (svm). because machine learning methods are rarely used by applied biostatisticians, the primary goal of this paper is to explain the concept of probability estimation with these methods and to summarize recent theoretical findings. probability estimation in k-nn, b-nn, and rf can be embedded into the class of nonparametric regression learning machines; therefore, we start with the construction of nonparametric regression estimates and review results on consistency and rates of convergence. in svms, outcome probabilities for individuals are estimated consistently by repeatedly solving classification problems. for svms we review classification problem and then dichotomous probability estimation. next we extend the algorithms for estimating probabilities using k-nn, b-nn, and rf to multicategory outcomes and discuss approaches for the multicategory probability estimation problem using svm. in simulation studies for dichotomous and multicategory dependent variables we demonstrate the general validity of the machine learning methods and compare it with logistic regression. however, each method fails in at least one simulation scenario. we conclude with a discussion of the failures and give recommendations for selecting and tuning the methods. applications to real data and example code are provided in a companion article (doi:10.1002/bimj.201300077).'}, 'keywords': [['Bagged nearest neighbor', 'Nonparametric regression', 'Probability estimation', 'Random forest', 'Support vector machine']]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'papers.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now 'data' contains the JSON data as a Python object\n",
    "# For example, print an item to check\n",
    "print(data[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db2567",
   "metadata": {},
   "source": [
    "## **Create LangChain Document objects**\n",
    "A Document object contains page_content (str) and metadata (dict). This object will be useful for splitting large documents, into smaller chunks later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07549250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "transformed_docs = []\n",
    "counter = 1  # Start the counter\n",
    "\n",
    "for doc in data:\n",
    "    title = doc['title']['full_text']\n",
    "    abstract = doc['abstract']['full_text']\n",
    "    unique_id = f\"pubmed-{counter:07d}\"  # Format the ID with leading zeros\n",
    "\n",
    "    if doc['keywords'] and isinstance(doc['keywords'][0], list):\n",
    "        keywords = doc['keywords'][0]\n",
    "    else:\n",
    "        keywords = []\n",
    "\n",
    "    document = Document(\n",
    "        page_content=abstract,\n",
    "        metadata={\n",
    "            'title': title,\n",
    "            'keywords': keywords,\n",
    "            'unique_id': unique_id  # Use the formatted unique ID\n",
    "        }\n",
    "    )\n",
    "    transformed_docs.append(document)\n",
    "    counter += 1  # Increment the counter for the next document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131bf478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='', metadata={'title': 'efficient use of social media during the avian influenza a(h7n9) emergency response.', 'keywords': [], 'unique_id': 'pubmed-0000001'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f054154",
   "metadata": {},
   "source": [
    "## **Split the documents**\n",
    "We now split large documents into smaller pieces, and create a new Document object for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3167c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunked_texts = []\n",
    "\n",
    "for doc in transformed_docs:\n",
    "    # Use 'split_text' to split the document's page_content into chunks\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Create a new Document for each chunk, preserving the original metadata\n",
    "        chunked_doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata=doc.metadata  # This includes the unique_id\n",
    "        )\n",
    "        chunked_texts.append(chunked_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88101afd",
   "metadata": {},
   "source": [
    "## **Embedding and Storage using OpenAIEmbeddings and Chroma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6a730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings,GPT4AllEmbeddings,HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a7d37a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './Chroma/chroma_openai' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "persist_directory = \"./Chroma/chroma_openai\"\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "    print(f\"Directory '{persist_directory}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{persist_directory}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266ad4c",
   "metadata": {},
   "source": [
    "### **Prepare batch system for embedding**\n",
    "To not embed all chunked documents at once, we prepare a batching system to reduce the resource load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18634a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(docs_batch, embedding, persist_directory):\n",
    "    #embedding.set_runtime(\"gpu\")\n",
    "    simplified_docs_batch = []\n",
    "    for doc in docs_batch:\n",
    "        # Convert list of keywords to a single string\n",
    "        keywords_str = ', '.join(doc.metadata.get('keywords', []))\n",
    "\n",
    "        # Create new metadata with simple data types\n",
    "        simplified_metadata = {\n",
    "            'title': doc.metadata.get('title', ''),\n",
    "            'keywords': keywords_str\n",
    "        }\n",
    "\n",
    "        # Create a new Document with simplified metadata\n",
    "        simplified_doc = Document(page_content=doc.page_content, metadata=simplified_metadata)\n",
    "        simplified_docs_batch.append(simplified_doc)\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=simplified_docs_batch, embedding=embedding, persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_process_embeddings(docs, batch_size, embedding, persist_directory):\n",
    "    for i in tqdm(range(0, len(docs), batch_size)):\n",
    "        docs_batch = docs[i:i + batch_size]\n",
    "        process_batch(docs_batch, embedding, persist_directory)\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc32ac",
   "metadata": {},
   "source": [
    "### **Execute embedding and store to Chroma.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0868349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09fbc57a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunked_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Chroma/chroma_openai\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m batch_process_embeddings(\u001b[43mchunked_texts\u001b[49m, batch_size, OpenAIEmbeddings(), persist_directory)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunked_texts' is not defined"
     ]
    }
   ],
   "source": [
    "persist_directory = './Chroma/chroma_openai'\n",
    "batch_process_embeddings(chunked_texts, batch_size, OpenAIEmbeddings(), persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45f33ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: page_content='in this work, we theoretically propose an optical biosensor (consists of a bk7 glass, a metal film, and a graphene sheet) based on photonic spin hall effect (she). we establish a quantitative relationship between the spin-dependent shift in photonic she and the refractive index of sensing medium. it is found that, by considering the surface plasmon resonance effect, the refractive index variations owing to the adsorption of biomolecules in sensing medium can effectively change the spin-dependent displacements. remarkably, using the weak measurement method, this tiny spin-dependent shifts can be detected with a desirable accuracy so that the corresponding biomolecules concentration can be determined.' metadata={'keywords': '', 'seq_num': 38643, 'source/title': 'photonic spin hall effect enabled refractive index sensor using weak measurements.'}\n",
      "Score: 0.24147269129753113\n",
      "\n",
      "Document: page_content='over the last 30 years, optical biosensors based on nanostructured materials have obtained increasing interest since they allow the screening of a wide variety of biomolecules with high specificity, low limits of detection, and great sensitivity. among them, flexible optical platforms have the advantage of adapting to non-planar surfaces, suitable for in vivo and real-time monitoring of diseases and assessment of food safety. in this review, we summarize the newest and most advanced platforms coupling optically active materials (noble metal nanoparticles) and flexible substrates giving rise to hybrid nanomaterials and/or nanocomposites, whose performances are comparable to the ones obtained with hard substrates (e.g., glass and semiconductors). we focus on localized surface plasmon resonance (lspr)-based and surface-enhanced raman spectroscopy (sers)-based biosensors. we show that large-scale, cost-effective plasmonic platforms can be realized with the currently available techniques' metadata={'keywords': 'LSPR-based biosensors, SERS-based biosensors, disease early-diagnosis, flexible hybrid materials, nanocomposite materials, nanofabrication techniques, optical biosensors', 'seq_num': 95411, 'source/title': 'recent advances in the fabrication and functionalization of flexible optical biosensors: toward smart life-sciences applications.'}\n",
      "Score: 0.2425813376903534\n",
      "\n",
      "Document: page_content=\"such optical conversions provide unique opportunities for biosensing as they recapitulate the topography of the extracellular matrix. this creates a wide array of potential theranostic, fiber-based applications in disease diagnosis/imaging, drug delivery and monitoring of therapeutic response. using a fiber-based vehicle, we observed gaseous oxygen sensing capabilities and a linear stern-volmer response allowing highly accurate calibration. configurational aspects were also studied to determine how to maximize the efficiency of this 'handshake' interaction.\" metadata={'keywords': 'Electrospinning, In vivo oxygen sensing, Polycaprolactone, Polysulfone, Upconversion, Upconverting nanoparticles', 'seq_num': 26309, 'source/title': 'nanoscale upconversion for oxygen sensing.'}\n",
      "Score: 0.25551044940948486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the proposed optical biosensor based on?\"\n",
    "\n",
    "persist_directory = './Chroma/chroma_openai'\n",
    "db3 = Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings())\n",
    "# Call the similarity search method with the query and k\n",
    "docs = db3.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for doc, score in docs:\n",
    "    print(f\"Document: {doc}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579caa51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

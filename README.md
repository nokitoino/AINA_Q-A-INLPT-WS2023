# Medical QA system chatbot (NLP with Transformer Project)

Introduction:

Nowadays, we have opened LLMs (like chatgpt, Bard,Huggingchat…) and the list goes on in order to search for answers to your queries, but most of them are not up-to-date and most importantly, they are not domain-specific. (as in our case, all medical-related information).
Even though they do provide information, the level of precision and accuracy is the point of the question. Most of these LLMs hallucinate most of the answers if they don't know the answer.
Most of the LLMs function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.
Hence, we thought of introducing (a smart retrieval system similar to RAG, which can be combined with LLM(to generate the text).
By doing so, we are more confident about the answers given now by LLM as they are generated by our own corpus of data.

High-end architecture:

![project_QA_arch](https://github.com/nokitoino/_Q-A-INLPT-WS2023/assets/35266536/1d1f21cd-3d71-4c81-a633-87bfaae8dd61)

There are two phases to the project architecture:
Retrieval of the relevant documents 
Generation of new text from the documents 

Phase 1: Data Retrival
Step 1: Webscrape the content (title, abstract,authors,references) from PubMed website (https://pubmed.ncbi.nlm.nih.gov/?term=intelligence+%5BTitle%2Fabstract%5D&filter=simsearch1.fha&filter=years.2014-2024&sort=date) from 2014-2024 and store it in JSON format.
Step 2: There are 2 options:
1st option : The bulk upload of the json file into Elasticsearch performing customized mappings performing semantic search and providing the relevant documents along with similarity score 
2nd option : Perform chunking using (fill it up)
Step 3: Using the OpenAI embedding API, embed the content and store it in the vector store.
Phase 2: Generation phase 
Step 1: when the user posts a question on UI (steamlit), it is converted into an embedded API 
Step 2: The question posted will be paired up with the vectors in the vector store using semantic search, and the most relevant documents (k value = number of relevant documents) with respect to the questions will be provided to LLM 
Step 3:These documents will be sent in context to the LLM (Medapeca) along with questions in order to generate most suitable answer and displayed in UI with references( most relevant documents).

Copyright (c) 2023.

This code has been developed as part of the "NLP with Transformer Project" project.

Teamates contribution(as of now):
1. Anu Reddy:
   - Performed data analysis on the documents from pubmed url link.
   - Performed data retrieval on the combined documents in ndjson format (provided by akin) via writing queries in elasticsearch in Python( and cross-verified in kibana) and optimized the queries to retrieve the documents using both keyword and vector search on the textual components of the dataset(title and abstract).
   - Performed some test queries by providing the questions as input and retrieving the corresponding documents along with their cosine similarity scores.
2. Akin Yilmaz:
   - Developed the PudMedScraper.py using Entrez. Bypassed the Ratelimit of 9999 files using date intervalls. Created the JSON format in cooperation with the others.
   - Testing a parallel Pipeline using Haystack for the entire workflow. So far implemented the DocumentStore using Elasticsearch and experimented with the TF-IDF (sparse) Retriever.
## Requirements

### Software Dependencies
- libA (version X.Y.Z)
- libB (version A.B.C)
- ...

### Supported Operating Systems
- macOS
- Linux
- Windows
- ...
  
(Add any notes about the platforms or setups used for development & testing.)

## Installation & Build

_General dependency & build install instructions with per-platform subsections when needed. Please include commandline steps, when applicable._

To get started, follow these steps:
1. Download and install [Anaconda](https://www.anaconda.com/products/individual).
2. Create a virtual environment:
```shell
$ conda create -n "name" python=3.10
$ conda activate "name"
$ pip install -r requirements.txt
```
3. Install project dependencies
```shell
$ pip install -r requirements.txt
```
(Include any platform-specific instructions if needed.)

## Usage
After installation, you can use the application as follows:

### Show Help
```
$ python executable.py --help
```

### Scenario 1
```
$ python executable.py --parameter 42
```

(Provide more usage examples if applicable.)

## References / Further Reading
Here are some useful references and related projects:
- [Search with Meanings: An Overview of Semantic Search Systems]([http://www.bargiela.com/papers/a35.pdf])
- [Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems]([https://arxiv.org/abs/2309.06384])
- [HuggingFace's Transformers: State-of-the-art Natural Language Processing] ([https://arxiv.org/abs/1910.03771])

## Contribute
Contributions are very welcome! Here's how to get involved:

1. Clone or fork the repository.
2. Make your changes or improvements.
3. Create a pull request.
4. If you find any bugs or have suggestions, please log them here as well.

(Include additional details on development environment setup, coding style, testing, and issue reporting as needed.)

---



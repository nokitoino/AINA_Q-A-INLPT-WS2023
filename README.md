Github UserName Mapping

| UserName     | StudentName | Matrikelnummer     |
|----------|-----|----------------|
|   Ibinmbiju   | Ibin Mathew Biju  | 3770662 |
|   AnuR1234  | Anu Reddy | 3768482  |
|   nokitoino   | Burhan Akin Yilmaz | 4114082    |
|   nr59684   | Nilesh Parshotam Rijhwani | 3771253    |

# Medical QA system chatbot (NLP with Transformer Project)

Introduction:

Nowadays, we have opened LLMs (like chatgpt, Bard,Huggingchat…) and the list goes on in order to search for answers to your queries, but most of them are not up-to-date and most importantly, they are not domain-specific. (as in our case, all medical-related information).
Even though they do provide information, the level of precision and accuracy is the point of the question. Most of these LLMs hallucinate most of the answers if they don't know the answer.
Most of the LLMs function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.
Hence, we thought of introducing (a smart retrieval system similar to RAG, which can be combined with LLM(to generate the text).
By doing so, we are more confident about the answers given now by LLM as they are generated by our own corpus of data.

High-end architecture:

![project_QA1](https://github.com/nokitoino/_Q-A-INLPT-WS2023/assets/35266536/1467df8a-d646-424e-9cf4-2670c4cbeb02)

There are two phases to the project architecture:
Retrieval of the relevant documents 
Generation of new text from the documents 

Phase 1: Data Retrival
Step 1: Webscrape the content (title, abstract,authors,references) from PubMed website (https://pubmed.ncbi.nlm.nih.gov/?term=intelligence+%5BTitle%2Fabstract%5D&filter=simsearch1.fha&filter=years.2014-2024&sort=date) from 2014-2024 and store it in JSON format.
Step 2: There are 2 options:
1st option : The bulk upload of the json file into Elasticsearch performing customized mappings performing semantic search and providing the relevant documents along with similarity score 
2nd option : Perform chunking using (fill it up)
Step 3: Using the OpenAI embedding API, embed the content and store it in the vector store.
Phase 2: Generation phase 
Step 1: when the user posts a question on UI (steamlit), it is converted into an embedded API 
Step 2: The question posted will be paired up with the vectors in the vector store using semantic search, and the most relevant documents (k value = number of relevant documents) with respect to the questions will be provided to LLM 
Step 3:These documents will be sent in context to the LLM (Medapeca) along with questions in order to generate most suitable answer and displayed in UI with references( most relevant documents).

Copyright (c) 2023.

This code has been developed as part of the "NLP with Transformer Project" project.

Teamates contribution(as of now):
1. Anu Reddy:
   - Performed data analysis on the documents from pubmed url link.
   - Performed data retrieval on the combined documents in ndjson format (provided by akin) via writing queries in elastic search in Python( and cross-verified in kibana) and optimized the queries to retrieve the documents using both keyword and vector search on the textual components of the dataset(title and abstract).
   - Performed some test queries by providing the questions as input and retrieving the corresponding documents along with their cosine similarity scores.
   - Experimented with different Embeddings models (openAI,BGE embeddings) and the comparison report has been shared(https://docs.google.com/document/d/1oVKGwl1XahiJP7jK8ojgg4UXNGZ2AUEMDeUFgmyyIZQ/edit?usp=sharing)
   - Worked with Hybrid search (Ensemble Retriver = BM25 retriever+Faiss retriever) (implementation will be uploaded soon)
   - Experimented with usage of different LLMs (openAI,Huggingfacehub(HuggingFaceH4/zephyr-7b-beta) and bloke model Medalpeca(medical LLM) to generate the context (implementation will be uploaded soon)
   - Looking to different evaluating metrics 
2. Akin Yilmaz:
   - Developed the PudMedScraper.py using Entrez. Bypassed the Ratelimit of 9999 files using date intervalls. Created the JSON format in cooperation with the others.
   - Testing a parallel Pipeline using Haystack for the entire workflow. Implemented simple pipeline using the DocumentStore using Elasticsearch and experimented with the TF-IDF (sparse) Retriever, and T5 as LLM, aborted the continuation due to the group agreement to stick to LangChain.
   - Implemented and commented embedding_evaluation.py
   - Only Commented and uploaded Embedding-OpenAI-Chroma.ipynb, which was implemented by Anu Reddy.
   - Implemented LLM-GPT3.5-Turbo.ipynb, which is the continuation of our base Notebook Embedding-OpenAI-Chroma.ipynb. It uses GPT3.5 Turbo as LLM.
   - Implemented Evaluation-Contextual-Compression.ipynb based on the ideas used in the lectures last assignment. Uses LLMExtractor, and different metrics to evaluate the performance of our entire pipeline with the help of Hugging Face pubmed_qa dataset.
3. Nilesh Rijhwani:
   - Working on the automation of webscrapping where I am using following structure to maintain timely webscarpping:
      - Using Python scheduler library - APScheduler
      - Also storing the last scrapped date in a text file amd using functions to access and update the same before putting it in query.
   - Defined a chunk size of 1000 to start working on the experimental phase where the inout json from the webscrapping (papers.json) is brokendown in to defined chunksize and stored as json which later will be used in ambedding.py as an input to our ambedding function
   - Working on embedding model and function, current decision - 'text_davinci_003' which is gonna be deprectaed in jan 2024, next model --> gpt-3.5-turbo-1106
      - Implemented the Embedding model for PubMed Documents using OpenAI's GPT-3 API.
4. Ibin Mathew Biju:
   - Experimented with different embedding models such as openAI and compared its performance.
   - Researched and worked on different vector stores such as FAISS, chromaDB, pinecone to figure out the best suitable vector store for the architecture.
   - Implemented FAISS vector store and integrated with the current embedding files and did vector search.
   - Currently working on different LLMs and experimenting different combinations for better results.
## Requirements

### Software Dependencies
- libA (version X.Y.Z)
- libB (version A.B.C)
- ...

### Supported Operating Systems
- macOS
- Linux
- Windows
- ...
  
(Add any notes about the platforms or setups used for development & testing.)

## Installation & Build

_General dependency & build install instructions with per-platform subsections when needed. Please include commandline steps, when applicable._

To get started, follow these steps:
1. Download and install [Anaconda](https://www.anaconda.com/products/individual).
2. Create a virtual environment:
```shell
$ conda create -n "name" python=3.10
$ conda activate "name"
$ pip install -r requirements.txt
```
3. Install project dependencies
```shell
$ pip install -r requirements.txt
```
(Include any platform-specific instructions if needed.)

## Usage
After installation, you can use the application as follows:

### Show Help
```
$ python executable.py --help
```

### Scenario 1
```
$ python executable.py --parameter 42
```

(Provide more usage examples if applicable.)

## References / Further Reading
Here are some useful references and related projects:
- [Search with Meanings: An Overview of Semantic Search Systems]([http://www.bargiela.com/papers/a35.pdf])
- [Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems]([https://arxiv.org/abs/2309.06384])
- [HuggingFace's Transformers: State-of-the-art Natural Language Processing] ([https://arxiv.org/abs/1910.03771])

## Contribute
Contributions are very welcome! Here's how to get involved:

1. Clone or fork the repository.
2. Make your changes or improvements.
3. Create a pull request.
4. If you find any bugs or have suggestions, please log them here as well.

(Include additional details on development environment setup, coding style, testing, and issue reporting as needed.)

---



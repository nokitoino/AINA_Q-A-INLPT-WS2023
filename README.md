
This project has been developed as part of the "Natural Language Processing with Transformers" lecture, conducted by Professor Gertz.


Github UserName Mapping

| UserName     | StudentName | Matrikelnummer     |
|----------|-----|----------------|
|   Ibinmbiju   | Ibin Mathew Biju  | 3770662 |
|   AnuR1234  | Anu Reddy | 3768482  |
|   nokitoino   | Burhan Akin Yilmaz | 4114082    |
|   nr59684   | Nilesh Parshotam Rijhwani | 3771253    |

# Medical QA system chatbot (NLP with Transformer Project)

Introduction:

Nowadays, we have opened LLMs (like chatgpt, Bard,Huggingchat…) and the list goes on in order to search for answers to your queries, but most of them are not up-to-date and most importantly, they are not domain-specific. (as in our case, all medical-related information).
Even though they do provide information, the level of precision and accuracy is the point of the question. Most of these LLMs hallucinate most of the answers if they don't know the answer.
Most of the LLMs function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.
Hence, we thought of introducing (a smart retrieval system similar to RAG, which can be combined with LLM(to generate the text).
By doing so, we are more confident about the answers given now by LLM as they are generated by our own corpus of data.

## Table of Contents
- [High-end Architecture](#high-end-architecture)
- [Documentation](#documentation)
- [Contribution](#contribution)
- [File Overview](#file-overview)
- [Installation](#installation)
- [Usage](#usage)
- [License](#license)
  
## High-end architecture:
![Unbenanntes_Diagramm drawio](https://github.com/nokitoino/_Q-A-INLPT-WS2023/assets/18616498/74372a75-16e1-4c3c-b87d-c176e97225ba)


## Documentation

The documentations can be found in [DOCUMENTATION.md](DOCUMENTATION.md), which contains the following content table.
- [Members Contribution](DOCUMENTATION.md)
- [RAG Pipeline](DOCUMENTATION.md)
- [Experimental Setup](DOCUMENTATION.md)
- [Hardware Requirements](DOCUMENTATION.md)
- [Evaluation](DOCUMENTATION.md)
- [Medical QA-system Demo](DOCUMENTATION.md)
- [Resources](DOCUMENTATION.md)

## File Overview, Installation, Usage

The [Scraper](LangChainRAG/) folder contains the document scraper, which is important to execute before using any other scripts from us.

| File     |  Functionality | Requirements
|----------|-----|----------------|
|   PubMedScraper.py  | Creates papers.json   | None |

The [LangChain RAG](LangChainRAG/) folder contains base Jupyter Notebooks that our final app is based on. It contains the embedding, vector storage, LLM, Contextual Compression.

| File     |  Functionality | Requirements
|----------|-----|----------------|
|   Embedding-OpenAI-Chroma.ipynb   | Creates OpenAI Chroma vector database ~ several hours to execute, 5 GB size. Alternatively, download here: [Download Chroma database](https://www.dropbox.com/scl/fi/237x8upy01vy8v6kw7h9i/Chroma.zip?rlkey=0dga7zqksbz22pwq1sqzkj02f&dl=0) | None |
|   Hybrid-Search-Contextual-Compression.ipynb   |Takes question, retrieves relevant docs with hybrid search, does contextual compression, generates answer with LLM | Chroma database & papers.json |
|   LLM-GPT3.5-Turbo.ipynb   | Uses only Chroma database and invokes LLM on question|  Chroma database & papers.json |

The [Evaluation](Evaluation/) folder contains scripts to assists/do the evaluation of embeddings, LLMs, and contextual compressions.
| File     |  Functionality | Requirements
|----------|-----|----------------|
|   Evaluate_embeddings_HIT_MRR.ipynb  | Evaluates multiple embeddings using HIT MMR | PubMed QA dataset |
|   Evaluation-Contextual-Compression.ipynb  | Evaluates contextual compression impact | Chroma database & papers.json & (QA dataset, we use PubMed QA dataset) |
|    qa_generator.py   | Generates automatically questions for each document in papers.json |  papers.json |



The [Follow-up Questions](Follow-up-Questions/) folder contains the implementation to generate follow up questions.

| File     |  Functionality | Requirements
|----------|-----|----------------|
|   Generate_followup_ques_chroma.ipynb  | Generates follow up questions | Chroma database, papers.json |


The [images](images/) folder contains snapshots of our demo.

The [Archieves](Archieves/) folder contains omitted approaches.


The [Frontend-Streamlit](Frontend-Streamlit/) folder contains the user interface and the entire backend.

| File     |  Functionality | Requirements
|----------|-----|----------------|
|   app.py  | End-To-End UI | Chroma database, papers.json |

## Installation of our final QA-system

  ### Test the model in browser:
  1. Join the HuggingSpace Organization : [Huggingface Organization](https://huggingface.co/organizations/inltp-group20/share/sTBJmwoxoUamGbTXfJnIeqAEtsyqAggWgg)
  2. Test the model here : [PubMed Model](https://huggingface.co/spaces/inltp-group20/inltp_group20_pubmed_model) (might take around 3 minutes to generate answers)

  ### Run the app in localhost:
  1. Clone the repo : git clone https://github.com/nokitoino/_Q-A-INLPT-WS2023.git
  2. Download the chroma_openai embeddings file (5.8 GB) from here : [chroma_openai](https://www.dropbox.com/scl/fi/237x8upy01vy8v6kw7h9i/Chroma.zip?rlkey=0dga7zqksbz22pwq1sqzkj02f&dl=0)
  3. Navigate to ```Frontend-Stremlit/Hybrid_search.py```
  4. Change the path of ```'./chroma_openai'``` to the new path.
  5. Add the *OPENAI_API_KEY*
  6. Inside terminal run ```pip install streamlit```
  7. Run the app by ```streamlit run app.py```   (Give path to the app.py)



## License

This project is licensed under the [GNU General Public License (GPL) version 3](LICENSE.md) - see the [LICENSE.md](LICENSE.md) file for details.


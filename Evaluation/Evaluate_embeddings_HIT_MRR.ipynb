{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# OpenAI Embeddings Class remains unchanged\n",
    "class OpenAIEmbedding:\n",
    "    def __init__(self, dimensions=3072):\n",
    "        self.embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=1024)\n",
    "        \n",
    "    def encode(self, text):\n",
    "        embeddings = self.embedding_model.embed_documents([text])\n",
    "        return np.array(embeddings[0])\n",
    "\n",
    "class HuggingFaceEmbedding:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.to(\"cuda\")\n",
    "        \n",
    "    def encode(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            pooled_output = outputs.pooler_output if outputs.pooler_output is not None else outputs.last_hidden_state.mean(dim=1)\n",
    "        return pooled_output.cpu().numpy().squeeze()  # Move back to CPU for numpy compatibility\n",
    "        \n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Evaluation function updated with tqdm and execution time measurement\n",
    "\n",
    "def evaluate_embeddings(dataset, embeddings, k=1):\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    for name, embedding_model in embeddings.items():\n",
    "        hit_rates = []\n",
    "        mrr_scores = []\n",
    "        for example in tqdm(dataset, desc=f\"Evaluating {name}\"):\n",
    "            question_embedding = embedding_model.encode(example['question'])\n",
    "            contexts_embeddings = [embedding_model.encode(context) for context in example['context']['contexts']]\n",
    "            scores = [cosine_similarity(question_embedding, ctx_emb) for ctx_emb in contexts_embeddings]\n",
    "            sorted_scores_idx = np.argsort(scores)[::-1]\n",
    "            correct_answer_rank = sorted_scores_idx.tolist().index(0) + 1\n",
    "            \n",
    "            if correct_answer_rank <= k:\n",
    "                hit_rates.append(1)\n",
    "            else:\n",
    "                hit_rates.append(0)\n",
    "                \n",
    "            mrr_scores.append(1 / correct_answer_rank)\n",
    "            \n",
    "        hit_rate = np.mean(hit_rates)\n",
    "        mrr = np.mean(mrr_scores)\n",
    "        results.append({\"Embedding\": name, \"Hit Rate\": hit_rate, \"MRR\": mrr})\n",
    "    print(f\"Total evaluation time: {time.time() - start_time:.2f} seconds\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split='train[:1000]')  \n",
    "embeddings = {\n",
    "    \"OpenAI\": OpenAIEmbedding(dimensions=1024),\n",
    "    \"bge-large\": HuggingFaceEmbedding(model_name='BAAI/bge-large-en'),\n",
    "    \"pubmedbert\": HuggingFaceEmbedding(model_name='bert-base-uncased')\n",
    "}\n",
    "\n",
    "# Evaluation and display section remains unchanged\n",
    "\n",
    "eval_results = evaluate_embeddings(dataset, embeddings, k=5)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating OpenAI: 100%\n",
    " 1000/1000 [18:59<00:00,  1.15it/s]\n",
    "Evaluating bge-large: 100%\n",
    " 1000/1000 [01:56<00:00,  9.20it/s]\n",
    "Evaluating pubmedbert: 100%\n",
    " 1000/1000 [00:52<00:00, 20.04it/s]\n",
    "Total evaluation time: 1308.37 seconds\n",
    "\n",
    "Embedding\tHit Rate\tMRR\n",
    "0\tOpenAI\t1.000\t0.849433\n",
    "1\tbge-large\t0.986\t0.588398\n",
    "2\tpubmedbert\t0.985\t0.550596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

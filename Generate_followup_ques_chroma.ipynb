{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92cd56f3",
   "metadata": {},
   "source": [
    "# **Advanced RAG Technique and Evaluation**\n",
    "\n",
    "In this document we will try the standard RAG versus the compressor based RAG.\n",
    "We use GPT3.5 Turbo as LLM and will use as retriever a contextual compressor, which only takes the most relevant information from the retrieved documents by the similarity search.\n",
    "\n",
    "Requirements: Please make sure to execute first LangChainRAG/Embedding-OpenAI-Chroma.ipynb to embed our medical documents. This Notebook is merely applying GPT3.5 Turbo as LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdd4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bca64d",
   "metadata": {},
   "source": [
    "## **Load Chroma and GPT3.5 Turbo LLM**\n",
    "We first load the Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6a730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings,GPT4AllEmbeddings,HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7d37a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'E:\\NLPT\\_Q-A-INLPT-WS2023\\chroma_openai-003\\chroma_openai' exists, perfect!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "persist_directory = \"E:\\\\NLPT\\\\_Q-A-INLPT-WS2023\\\\chroma_openai-003\\\\chroma_openai\"\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(persist_directory):\n",
    "    print(f\"Please execute first LangChainRAG/Embedding-OpenAI-Chroma.ipynb, we didn't find any Chroma vector storage.\")\n",
    "else:\n",
    "    print(f\"Directory '{persist_directory}' exists, perfect!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0046687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        os.environ['OPENAI_API_KEY'] = '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'\n",
    "        self.embedding = OpenAIEmbeddings()\n",
    "        self.ensemble_retriever = None\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def initialize_bm25_retriever(self, docs):\n",
    "        if not all(isinstance(doc, Document) for doc in docs):\n",
    "            raise ValueError(\"All items in docs must be Document instances.\")\n",
    "        abstracts = [doc.page_content for doc in docs]\n",
    "        bm25_retriever = BM25Retriever.from_texts(abstracts, metadatas=[doc.metadata for doc in docs])\n",
    "        bm25_retriever.k = 3\n",
    "        return bm25_retriever\n",
    "\n",
    "    def transform_data_to_documents(self, data):\n",
    "        docs = []\n",
    "        for doc in data:\n",
    "            title = doc.get('title', {}).get('full_text', '')\n",
    "            abstract = doc.get('abstract', {}).get('full_text', '')\n",
    "            keywords = doc.get('keywords', [[]])[0] if doc['keywords'] and isinstance(doc['keywords'][0], list) else []\n",
    "            document = Document(page_content=abstract, metadata={'title': title, 'keywords': keywords})\n",
    "            docs.append(document)\n",
    "        return docs\n",
    "\n",
    "    def process_documents_with_chroma(self, docs):\n",
    "        persist_directory = './Chroma/chroma_openai'\n",
    "        db3 = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n",
    "        if not all(isinstance(doc, Document) for doc in docs):\n",
    "            raise ValueError(\"All items in docs must be Document instances.\")\n",
    "        chroma_retriever = db3.as_retriever(search_kwargs={'k': 3})\n",
    "        return chroma_retriever\n",
    "\n",
    "    def create_ensemble_retriever(self, bm25_retriever, chroma_retriever):\n",
    "        # faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={'k': 10})\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever], weights=[0.7, 0.3])\n",
    "        self.ensemble_retriever = ensemble_retriever\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        print(results)\n",
    "        formatted_results = []\n",
    "        for document in results:\n",
    "            doc_info = {\n",
    "                'title': document.metadata.get('title', document.metadata.get('source/title','No Title')),\n",
    "                'keywords': document.metadata.get('keywords', []),\n",
    "                'abstract': document.page_content\n",
    "            }\n",
    "            formatted_results.append(doc_info)\n",
    "        return formatted_results\n",
    "\n",
    "hs = HybridSearch(\n",
    "        'E:\\\\NLPT\\\\_Q-A-INLPT-WS2023\\\\Transfromer_project-20240228T221604Z-002\\\\Transfromer_project\\\\data\\\\papers_latest.json')\n",
    "data = hs.load_data()\n",
    "docs = hs.transform_data_to_documents(data)\n",
    "bm25_retriever = hs.initialize_bm25_retriever(docs)\n",
    "\n",
    "chroma_vectorstore = hs.process_documents_with_chroma(docs)\n",
    "hs.create_ensemble_retriever(bm25_retriever, chroma_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f628b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "retriever = hs.ensemble_retriever #db3.as_retriever() # print(dir(db3)) to get all functions, attributes\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor,LLMChainFilter\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(\n",
    "    llm=llm\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "rag_chain_compressor = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be97bd2",
   "metadata": {},
   "source": [
    "## **Generate follow up Questions for compressed context**\n",
    "Here we use openai to only take the relevant context and generate followup questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40df6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, openai\n",
    "def generate_questions(title, context, amount):\n",
    "    '''We ask ChatGPT to form questions over our context, while taking into account different forms of questions.\n",
    "       1. Confirmation Question [yes or no]\n",
    "       2. Factoid-type Question [what, which, when, who, how]\n",
    "       3. List-type Question\n",
    "       4. Casual Question [why or how]\n",
    "       5. Hypothetical Question [what if]\n",
    "       6. Complex question (requires understanding of multiple texts)\n",
    "    '''\n",
    "    question_type = [\"confirmation questions [yes or no]\", \"factoid-type questions [what, which, when, who, how]\",\"list-type questions\",\"casual questions [why or how]\",\"hypothethical questions [e.g. what would happen if...]\",\"questions\"]\n",
    "    random_type = random.randint(0,len(question_type)-1)\n",
    "\n",
    "    prompt = f\"\"\"Context: {context}\\n\\nI want to evaluate my document embeddings. Form {question_type[random_type]} in your own words. Answer in this json format {{\"question_1\": '', ...}} (Exact Amount: {amount}), don't say anything else.\"\"\"\n",
    "    # Careful, when you add {title} in the prompt. ChatGPT tends to make the question title-specific\n",
    "\n",
    "    # You can adjust temperature and max tokens as per your preferences\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Use the chat model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "        n=2  # Generate three questions\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83a58d",
   "metadata": {},
   "source": [
    "## **Generate answers using contextual compression**\n",
    "Here we use LLMChainExtractor to only take the relevant information from each document. We prepare the compressor based retriever and generate answers in an analogous way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75c1b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rijhw\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rijhw\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rijhw\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "query = 'What is ease-mm ?'# 'How does the new prediction method, EASE-MM, select the final prediction model?''\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "#print(rag_chain_compressor.invoke(query))\n",
    "generated_questions = generate_questions(query, compressed_docs[0].page_content,amount=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e979f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"question_1\": \"How are document embeddings calculated for Ease-mm?\",\\n  \"question_2\": \"Why are document embeddings used in evaluating protein stability changes?\",\\n  \"question_3\": \"How do document embeddings help in predicting accurate protein stability changes induced by single amino acid substitutions?\"\\n}'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
